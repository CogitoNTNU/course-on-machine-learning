{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/CogitoNTNU/course-on-large-language-models/blob/main/Course_on_Large_Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course on Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy matplotlib scipy pandas seaborn scikit-learn statsmodels tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import plot_tree\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"[SUCCESS] Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning project cycle\n",
    "Machine learning is a process that involves several steps. The following is a typical machine learning project cycle:\n",
    "\n",
    "1. **Problem definition**: Define the problem you are trying to solve. This is the most important step in the machine learning project cycle. If you don't define the problem correctly, you will not be able to solve it.\n",
    "2. **Data collection**: Collect data that will be used to train the machine learning model. The data should be representative of the problem you are trying to solve.\n",
    "3. **Data preprocessing**: Clean the data and prepare it for training. This may involve removing missing values, normalizing the data, and encoding categorical variables.\n",
    "4. **Model selection**: Choose the machine learning model that will be used to solve the problem. There are many different machine learning models to choose from, and the best model will depend on the problem you are trying to solve.\n",
    "5. **Model training**: Train the machine learning model on the training data. This involves feeding the model the training data and adjusting the model's parameters to minimize the error.\n",
    "6. **Model evaluation**: Evaluate the performance of the machine learning model on the test data. This involves feeding the model the test data and measuring how well the model performs.\n",
    "7. **Model deployment**: Deploy the machine learning model in a production environment. This may involve integrating the model with other systems and monitoring the model's performance over time.\n",
    "8. **Model maintenance**: Maintain the machine learning model over time. This may involve retraining the model on new data and updating the model's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data: pd.DataFrame = pd.read_csv('data/train.csv')\n",
    "test_data: pd.DataFrame = pd.read_csv('data/test.csv')\n",
    "training_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: separate the features from the target\n",
    "The target is the variable we want to predict. The features are the variables we use to predict the target.\n",
    "It is common to use `X` to denote the features and `y` to denote the target. \n",
    "It is important to separate the features from the target before we start training the model, as we do not have access to the target when we are making real predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Name the target column \"Survived\" and store it in y\n",
    "target_column = 'Survived'\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "Exploratory data analysis (EDA) is the process of analyzing data to extract insights and patterns. EDA is an important step in the machine learning project cycle, as it helps us understand the data and identify any issues that need to be addressed before training the model.\n",
    "\n",
    "For visualization, we will use the `matplotlib` and `seaborn` libraries. These libraries provide a wide range of plotting functions that can be used to create informative and visually appealing plots.\n",
    "\n",
    "To get the full overview check out: [Overview of seaborn plotting functions](https://seaborn.pydata.org/tutorial/function_overview.html)\n",
    "\n",
    "## TASK 2.1: Does the gender of the passenger affect the survival rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Sex', y=target_column, data=training_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2.2: Does the \"TicketClass\" the passenger affect the survival rate?\n",
    "Try to plot the survival rate for each ticket class. If so why does it intuitively make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Plot the survival rate by \"TicketClass\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2.3: There are missing values in the \"Age\" feature. How do you think the age of the passengers affects the survival rate?\n",
    "The age of the passengers is an important feature that can affect the survival rate. However, the age feature contains missing values. We need to handle these missing values before we can use the age feature in the model.\n",
    "\n",
    "Discuss in the group how you think the age of the passengers affects the survival rate and how you would handle the missing values in the age feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=training_data, x='Age', hue=target_column, kde=True)\n",
    "plt.show()\n",
    "\n",
    "# Show the missing data\n",
    "print(\"The number of missing values in each column:\")\n",
    "print(training_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of data\n",
    "When we are working with data, we need to preprocess it before we can use it to train a model. This preprocessing can include:\n",
    "* **Handling Missing Values**:\n",
    "  * **Imputation**: Replace missing values with a specific value like the mean, median, mode, or a constant value. This method is chosen based on the nature of the data and the type of variable (continuous or categorical). `data['column_name'] = data['column_name'].fillna(data['column_name'].mean())`, or use `sklearn.impute.SimpleImputer` to replace missing values with the mean, median, or mode.\n",
    "  * **Deletion**: Remove rows or columns with missing values, typically used when the proportion of missing data is minimal or if the missing data is not random.\n",
    "\n",
    "* **Encoding categorical variables**:\n",
    "  * **One-Hot Encoding**: Convert categorical variables into a numerical representation. This is done by creating a binary column for each category in the categorical variable. `data = pd.get_dummies(data, columns=['column_name'])`, or use `sklearn.preprocessing.OneHotEncoder` to encode categorical variables.\n",
    "  * **Label Encoding**: Convert categorical variables into a numerical representation. This is done by assigning a unique integer to each category in the categorical variable. `data['column_name'] = data['column_name'].cat.codes`, or use `sklearn.preprocessing.LabelEncoder` to encode categorical variables.\n",
    "* **Feature Scaling**:\n",
    "  * Normalization (Min-Max Scaling): Scales the features to a fixed range, usually 0 to 1, or -1 to 1. \n",
    "* **Handling Outliers**:\n",
    "  * **Trimming**: Remove extreme values.\n",
    "  * **Capping**: Apply thresholds to limit the range of variable values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data = data.drop(columns=[target_column], errors='ignore')\n",
    "    # TODO: Implement feature engineering here\n",
    "    data = data.drop(columns=['Name'])\n",
    "    data = pd.get_dummies(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: dropna should be removed when you implement the feature engineering for missing data\n",
    "training_data = training_data.dropna()\n",
    "y = training_data[target_column]\n",
    "\n",
    "X = engineer_features(training_data)\n",
    "X_test = engineer_features(test_data)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3: Change the hyperparameters of the model and see how it affects the performance of the model.\n",
    "* Change `max_depth` to a different value and see how it affects the performance of the model. The `max_depth` parameter controls the maximum depth of the decision tree. A higher value of `max_depth` will result in a more complex decision tree, which may lead to overfitting.\n",
    "* Change `min_samples_split` to a different value and see how it affects the performance of the model. The `min_samples_split` parameter controls the minimum number of samples required to split an internal node. A higher value of `min_samples_split` will result in a simpler decision tree, which may lead to underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models\n",
    "The steps to building and using a model are:\n",
    "\n",
    "* **Define**: What type of model will it be? A decision tree? Some other type of model like Neural Networks? Some other parameters of the model type are specified too.\n",
    "* **Fit**: Capture patterns from provided data. This is the heart of modeling. `model.fit(X, y)`\n",
    "* **Predict**: Just what it sounds like `model.predict(X_val)`\n",
    "* **Evaluate**: Determine how accurate the model's predictions are.\n",
    "\n",
    "Here is an example of defining a decision tree model with scikit-learn and fitting it with the features and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model. Specify a number for random_state to ensure same results each run\n",
    "# To see more what the model is doing, set verbose=1 or 2\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=2)\n",
    "\n",
    "# TODO: Try a different model (Some of the models require different hyperparameters so you may need to adjust them in the next cell)\n",
    "# model = HistGradientBoostingClassifier(random_state=42, max_iter=1000, max_depth=2)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = 1\n",
    "print(f\"Making predictions for the following {amount} people:\")\n",
    "print(X_val.head(amount))\n",
    "print(\"The predictions are\")\n",
    "print(model.predict(X_val.head(amount)))\n",
    "\n",
    "# Get the accuracy of the model\n",
    "predictions = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model\n",
    "Fine-tuning a model involves adjusting the model's hyperparameters to improve its performance. Hyperparameters are parameters that are set before the model is trained and cannot be learned from the data. Examples of hyperparameters include the learning rate, the number of hidden layers in a neural network, and the number of trees in a random forest.\n",
    "The following are an example of hyperparameters for a decision tree model using scikit-learn GridSearchCV:\n",
    "* **Grid search**: Grid search involves searching through a predefined set of hyperparameters and selecting the set of hyperparameters that produces the best performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid search parameters\n",
    "# These can be found in the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300], # Number of trees in the forest\n",
    "    'max_features': ['sqrt', 'log2', None], # Number of features to consider at every split\n",
    "    'max_depth': [4, 6, 8, 10], # Maximum number of levels in tree\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'] # Function to measure the quality of a split\n",
    "}\n",
    "\n",
    "# Perform Grid Search Cross-Validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best parameters found by grid search: {best_params}')\n",
    "\n",
    "# Train the classifier with the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation set accuracy after tuning: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the submission file\n",
    "submission = pd.DataFrame()\n",
    "submission[\"Survived\"] = best_model.predict(X_test)\n",
    "submission.to_csv('titanic_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the model\n",
    "After training the model, it is important to understand how the model works and how it makes predictions. This can help us identify any issues with the model and improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "Some machine learning models provide a measure of feature importance, which indicates how much each feature contributes to the model's predictions. Feature importance can help us understand which features are most important for predicting the target variable and identify any features that are not useful for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Feature Importance\n",
    "feature_importances = best_model.feature_importances_\n",
    "features = X_train.columns\n",
    "df = pd.DataFrame({'feature': features, 'importance': feature_importances})\n",
    "df = df.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=df)\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 4.0 Interpret a decision tree model\n",
    "\n",
    "Create a hypothetical passenger and go down the decision tree to predict whether the passenger survived or not.\n",
    "\n",
    "The redder the node, the higher the probability of not surviving. The bluer the node, the higher the probability of surviving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single decision tree\n",
    "single_tree = best_model.estimators_[0]\n",
    "plt.figure(figsize=(50, 50))\n",
    "plot_tree(\n",
    "    single_tree,\n",
    "    filled=True,\n",
    "    feature_names=X_train.columns, \n",
    "    fontsize=30,\n",
    "    precision=2\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. The confusion matrix is a 2x2 matrix that contains four values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Survied\", \"Died\"], yticklabels=[\"Survived\", \"Died\"])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for the competition\n",
    "* **Feature Engineering**: Create new features from the existing features to improve the performance of the model.\n",
    "  * For example, you can create new features by combining existing features, transforming existing features, or extracting information from existing features.\n",
    "The Names of the passengers contain titles such as \"Mr.\", \"Mrs.\", \"Miss.\", \"Master\", \"Major\", etc. You can extract these titles and create a new feature called \"Title\" to improve the performance of the model.\n",
    "  * Maybe one can extract the family size from the \"SiblingAndSpousesOnBoard\" and \"ParentsAndChildrenOnBoard\" features and create a new feature called \"FamilySize\". \n",
    "  * The \"Age\" feature contains missing values. You can impute the missing values using the mean, median, or mode of the \"Age\" feature.\n",
    "* **Train Multiple Models**: Train multiple models and compare their performance to select the best model.\n",
    "  Examples of machine learning models that can be used for classification problems include:\n",
    "  \n",
    "  If you want to use a neural network, you can use the `tensorflow` or `keras` library to build a neural network model.\n",
    "  ```python\n",
    "  from tensorflow import keras\n",
    "  # Define the neural network architecture\n",
    "  nn_model = keras.Sequential([\n",
    "      keras.layers.Dense(10, input_shape=(X_train.shape[1],), activation='relu', name='Input_Layer'),\n",
    "      keras.layers.Dense(5, activation='relu', name='Hidden_Layer_1'),\n",
    "      keras.layers.Dense(1, activation='sigmoid', name='Output_Layer')\n",
    "  ])\n",
    "\n",
    "  # Compile the model with the chosen optimizer, loss function, and evaluation metric\n",
    "  nn_model.compile(\n",
    "      optimizer='adam',\n",
    "      loss='binary_crossentropy',\n",
    "      metrics=['accuracy']\n",
    "  )\n",
    "\n",
    "  # Display the model's architecture\n",
    "  nn_model.summary()\n",
    "\n",
    "  # Train the model on the training data\n",
    "  history = nn_model.fit(\n",
    "      X_train, y_train, \n",
    "      epochs=10,  # Number of epochs (iterations over the entire dataset)\n",
    "      batch_size=32,  # Number of samples per gradient update\n",
    "      validation_data=(X_val, y_val),  # Monitor the validation performance during training\n",
    "      verbose=1  # Print training progress\n",
    "  )\n",
    "\n",
    "  # Predict on the validation set\n",
    "  y_pred = nn_model.predict(X_val)\n",
    "\n",
    "  # Post-processing of predictions if needed (e.g., converting probabilities to binary labels)\n",
    "  y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "  # Evaluate the model's performance on the validation set\n",
    "  loss, accuracy = nn_model.evaluate(X_val, y_val)\n",
    "  print(f'Validation Loss: {loss:.4f}')\n",
    "  print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "  ```\n",
    "* **Hyperparameter Tuning**: Tune the hyperparameters of the model to improve its performance.\n",
    "* **Cross-Validation**: Use cross-validation to evaluate the performance of the model on different subsets of the data.\n",
    "* **Automated Machine Learning (AutoML)**: Use AutoML tools to automatically select the best model and hyperparameters for the problem. Examples of AutoML tools include `TPOT`, `Auto-sklearn`, [`H2O AutoML`](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html), and [`AutoGloun`](https://auto.gluon.ai/stable/index.html). I recommend using `AutoGloun` as it is easy to use and can provide state-of-the-art performance.\n",
    "* **Model Stacking**: Models learn in different ways and if two models are almost as accurate but have different predictions for some observations, you can improve the accuracy of the model by combining the predictions of the two models. This is called model stacking. You can improve the performance of the model by combining multiple models to improve the performance of the model. [Code example on model stacking](https://scikit-learn.org/stable/auto_examples/ensemble/plot_stack_predictors.html) [Read more about model stacking](https://developer.ibm.com/articles/stack-machine-learning-models-get-better-results/) ![Model Stacking](https://developer.ibm.com/developer/default/articles/stack-machine-learning-models-get-better-results/images/model_stacking_overview-4.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
